{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DkZJ3dxWr7-4"
      },
      "source": [
        "DEFINE SOME USEFUL FUNCTION AND IMPORTING LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "w_yHq57gr12D"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import seaborn as sns\n",
        "#import pingouin as pg\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
        "from sklearn.metrics import make_scorer\n",
        "from utils import *"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RcYAuJPmsA4w"
      },
      "source": [
        "FIND FEATURE IMPORTANCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "QcZlStgUsG1z"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'n_tokens_content': ['n_non_stop_words', 'n_unique_tokens'], 'n_unique_tokens': ['n_non_stop_unique_tokens', 'n_tokens_content', 'n_non_stop_words'], 'n_non_stop_words': ['n_tokens_content', 'n_unique_tokens'], 'n_non_stop_unique_tokens': ['n_unique_tokens'], 'kw_min_min': ['kw_max_max'], 'kw_max_min': ['kw_avg_min'], 'kw_avg_min': ['kw_max_min'], 'kw_min_max': ['kw_min_avg'], 'kw_max_max': ['kw_min_min'], 'kw_min_avg': ['kw_min_max'], 'kw_max_avg': ['kw_avg_avg'], 'kw_avg_avg': ['kw_max_avg'], 'self_reference_min_shares': ['self_reference_avg_sharess'], 'self_reference_max_shares': ['self_reference_avg_sharess'], 'self_reference_avg_sharess': ['self_reference_max_shares', 'self_reference_min_shares'], 'global_sentiment_polarity': ['rate_positive_words'], 'global_rate_negative_words': ['rate_negative_words'], 'rate_positive_words': ['rate_negative_words', 'global_sentiment_polarity'], 'rate_negative_words': ['rate_positive_words', 'global_rate_negative_words'], 'avg_negative_polarity': ['min_negative_polarity'], 'min_negative_polarity': ['avg_negative_polarity'], 'title_subjectivity': ['abs_title_sentiment_polarity'], 'abs_title_sentiment_polarity': ['title_subjectivity']}\n",
            "kw_avg_avg                    0.259361\n",
            "kw_max_avg                    0.227842\n",
            "self_reference_avg_sharess    0.193903\n",
            "self_reference_min_shares     0.181423\n",
            "self_reference_max_shares     0.170796\n",
            "data_channel_world            0.166275\n",
            "LDA_02                        0.155207\n",
            "data_channel_lifestyle        0.137921\n",
            "data_channel_entertainment    0.117396\n",
            "data_channel_socmed           0.113001\n",
            "global_subjectivity           0.112726\n",
            "weekday_saturday              0.109282\n",
            "kw_min_avg                    0.104967\n",
            "weekday_sunday                0.097623\n",
            "data_channel_tech             0.094187\n",
            "kw_avg_min                    0.093259\n",
            "kw_max_min                    0.092380\n",
            "num_hrefs                     0.090480\n",
            "num_imgs                      0.086005\n",
            "global_sentiment_polarity     0.081447\n",
            "num_keywords                  0.077557\n",
            "n_non_stop_unique_tokens      0.072598\n",
            "rate_negative_words           0.072180\n",
            "LDA_01                        0.071839\n",
            "global_rate_positive_words    0.071263\n",
            "LDA_03                        0.066014\n",
            "title_sentiment_polarity      0.059091\n",
            "kw_min_max                    0.058559\n",
            "rate_positive_words           0.056733\n",
            "max_positive_polarity         0.055917\n",
            "average_token_length          0.054841\n",
            "avg_positive_polarity         0.053024\n",
            "LDA_04                        0.050768\n",
            "weekday_wednesday             0.050451\n",
            "Name: shares, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "path_dev = \"./data/development.csv\"\n",
        "##  Importing Dataset\n",
        "df = pd.read_csv(path_dev)\n",
        "\n",
        "## Removing non relevant feature\n",
        "df.drop(['url','id','timedelta'], axis = 1, inplace = True)\n",
        "\n",
        "##  One-Hot Encoding\n",
        "df = pd.get_dummies(df, columns=['data_channel', 'weekday'])\n",
        "\n",
        "##  Studying Correlation between features in order to remove features highly correlated  using spearman method because the problem is non linear\n",
        "top_10_values = {}\n",
        "dictionary = {}\n",
        "correlation_matrix = df.corr(method = 'spearman')\n",
        "for el in df.columns:\n",
        "  top_10_values[el] = correlation_matrix[el].abs().nlargest(35)[1:]\n",
        "for el in df.columns:\n",
        "  for i in range(0,10):\n",
        "    if(top_10_values[el][i] > 0.7):\n",
        "      if el in dictionary:\n",
        "        dictionary[el].append(top_10_values[el].index[i])\n",
        "      else:\n",
        "        dictionary[el] = [top_10_values[el].index[i]]\n",
        "\n",
        "## Feature highly correlated and feature important to our target value\n",
        "print(dictionary)\n",
        "print(top_10_values[\"shares\"])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QVbQ66CHtavC"
      },
      "source": [
        "AFTER THIS STUDY, BY HAND WE ANALYIZE THE MOST CORRELATED FEATURES AND WE REMOVE THE ONE THAT ARE LESS IMPORTANT TO OUR TARGET VALUE\n",
        "\n",
        "This is our result:\n",
        "\n",
        "\n",
        "toRemove = {'n_non_stop_words', 'n_unique_tokens','kw_max_max','kw_max_min','kw_min_max','kw_max_avg','self_reference_min_shares','self_reference_max_shares',\n",
        "'global_sentiment_polarity','global_rate_negative_words','avg_negative_polarity','abs_title_sentiment_polarity'}\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TbO4B1TAuA84"
      },
      "source": [
        "{'n_tokens_content': ['n_non_stop_words', 'n_unique_tokens'], 'n_unique_tokens': ['n_non_stop_unique_tokens', 'n_tokens_content', 'n_non_stop_words'], 'n_non_stop_words': ['n_tokens_content', 'n_unique_tokens'], 'n_non_stop_unique_tokens': ['n_unique_tokens'], 'kw_min_min': ['kw_max_max'], 'kw_max_min': ['kw_avg_min'], 'kw_avg_min': ['kw_max_min'], 'kw_min_max': ['kw_min_avg'], 'kw_max_max': ['kw_min_min'], 'kw_min_avg': ['kw_min_max'], 'kw_max_avg': ['kw_avg_avg'], 'kw_avg_avg': ['kw_max_avg'], 'self_reference_min_shares': ['self_reference_avg_sharess'], 'self_reference_max_shares': ['self_reference_avg_sharess'], 'self_reference_avg_sharess': ['self_reference_max_shares', 'self_reference_min_shares'], 'global_sentiment_polarity': ['rate_positive_words'], 'global_rate_negative_words': ['rate_negative_words'], 'rate_positive_words': ['rate_negative_words', 'global_sentiment_polarity'], 'rate_negative_words': ['rate_positive_words', 'global_rate_negative_words'], 'avg_negative_polarity': ['min_negative_polarity'], 'min_negative_polarity': ['avg_negative_polarity'], 'title_subjectivity': ['abs_title_sentiment_polarity'], 'abs_title_sentiment_polarity': ['title_subjectivity']}\n",
        "\n",
        "\n",
        "35 most important features that are more likely to determine target value\n",
        "\n",
        "kw_avg_avg                    0.259361\n",
        "kw_max_avg                    0.227842\n",
        "self_reference_avg_sharess    0.193903\n",
        "self_reference_min_shares     0.181423\n",
        "self_reference_max_shares     0.170796\n",
        "data_channel_world            0.166275\n",
        "LDA_02                        0.155207\n",
        "data_channel_lifestyle        0.137921\n",
        "data_channel_entertainment    0.117396\n",
        "data_channel_socmed           0.113001\n",
        "global_subjectivity           0.112726\n",
        "weekday_saturday              0.109282\n",
        "kw_min_avg                    0.104967\n",
        "weekday_sunday                0.097623\n",
        "data_channel_tech             0.094187\n",
        "kw_avg_min                    0.093259\n",
        "kw_max_min                    0.092380\n",
        "num_hrefs                     0.090480\n",
        "num_imgs                      0.086005\n",
        "global_sentiment_polarity     0.081447\n",
        "num_keywords                  0.077557\n",
        "n_non_stop_unique_tokens      0.072598\n",
        "rate_negative_words           0.072180\n",
        "LDA_01                        0.071839\n",
        "global_rate_positive_words    0.071263\n",
        "LDA_03                        0.066014\n",
        "title_sentiment_polarity      0.059091\n",
        "kw_min_max                    0.058559\n",
        "rate_positive_words           0.056733\n",
        "max_positive_polarity         0.055917\n",
        "average_token_length          0.054841\n",
        "avg_positive_polarity         0.053024\n",
        "LDA_04                        0.050768\n",
        "weekday_wednesday             0.050451\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gFnKP7YKuauC"
      },
      "source": [
        "**preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "KHxYy4qntmvU"
      },
      "outputs": [],
      "source": [
        "## Initializing the dataset\n",
        "path_eva = \"./data/evaluation.csv\"\n",
        "path_dev = \"./data/development.csv\"\n",
        "df = pd.read_csv(path_dev)\n",
        "df_eval = pd.read_csv(path_eva)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.max_rows', 500)\n",
        "id_col = df_eval['id']\n",
        "\n",
        "\n",
        "##  Deleting silly columns\n",
        "df.drop(['url','id','timedelta'], axis = 1, inplace = True)\n",
        "df_eval.drop(['url','id','timedelta'], axis = 1, inplace = True)\n",
        "\n",
        "\n",
        "#   Remove strange outliers with some data with 0 values\n",
        "mask = (df['average_token_length'] == 0) & (df['n_tokens_content'] == 0)\n",
        "df.drop(df[mask].index, axis = 0, inplace = True)\n",
        "\n",
        "\n",
        "##Filling NaN values in num_imgs and num_videos using zero\n",
        "NaN_columns = ['num_imgs','num_videos']\n",
        "for el in NaN_columns:\n",
        "    mean_values = df.groupby('data_channel')[el].mean()\n",
        "    # Iterate over each group and fill NaN values with the corresponding mean\n",
        "    for group, mean in mean_values.items():\n",
        "      df.loc[df['data_channel'] == group, el] = df.loc[df['data_channel'] == group, el].fillna(0)\n",
        "for el in NaN_columns:\n",
        "    mean_values_eval = df.groupby('data_channel')[el].mean()\n",
        "    # Iterate over each group and fill NaN values with the corresponding mean\n",
        "    for group, mean in mean_values_eval.items():\n",
        "      df_eval.loc[df_eval['data_channel'] == group, el] = df_eval.loc[df_eval['data_channel'] == group, el].fillna(0)\n",
        "\n",
        "##  filling NaN values in num_keywords grouping by 'data_channel' and calculate the mean\n",
        "mean_values = df.groupby('data_channel')[\"num_keywords\"].mean()\n",
        "for group, mean in mean_values.items():\n",
        "  df.loc[df['data_channel'] == group, \"num_keywords\"] = df.loc[df['data_channel'] == group, \"num_keywords\"].fillna(mean)\n",
        "  df_eval.loc[df_eval['data_channel'] == group, \"num_keywords\"] = df_eval.loc[df_eval['data_channel'] == group, \"num_keywords\"].fillna(mean)\n",
        "\n",
        "##  Transforming several features into a normal distribution shape using logaritmic transformation\n",
        "logTransformation = ['n_tokens_content','num_hrefs','num_self_hrefs','num_imgs','num_videos','kw_max_min','kw_avg_min','kw_min_max','kw_min_avg','kw_max_max','kw_max_avg','kw_avg_avg','self_reference_min_shares','self_reference_max_shares']\n",
        "df[logTransformation] = np.log(1.001 + df[logTransformation])\n",
        "df_eval[logTransformation] = np.log(1.001 + df_eval[logTransformation])\n",
        "\n",
        "\n",
        "\n",
        "##  Z-score normalization of our data\n",
        "for el in df.columns:\n",
        "  if(el != \"shares\" and el != \"data_channel\" and el != \"weekday\"):\n",
        "    df[el] = (df[el] - df[el].mean()) / df[el].std()\n",
        "\n",
        "for el in df_eval.columns:\n",
        "  if(el != \"shares\" and el != \"data_channel\" and el != \"weekday\"):\n",
        "    df_eval[el] = (df_eval[el] - df[el].mean()) / df[el].std()\n",
        "\n",
        "\n",
        "\n",
        "##  One-Hot Encoding\n",
        "df = pd.get_dummies(df, columns=['data_channel','weekday'])\n",
        "df_eval = pd.get_dummies(df_eval, columns=['data_channel','weekday'])\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bPIgVAlRwo17"
      },
      "source": [
        "DROPPING REDUNDANT COLUMNS AND REMOVING OUTLIER MORE LIKELY TO PREDICT TARGET VALUE IN UNIVARIATE WAY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ua2CK_tmwu28",
        "outputId": "45079013-e362-4f46-8790-5906ee098cf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of outliers in \"shares : 825\n",
            "-13095.0\n",
            "16740.0\n",
            "1755.0\n",
            "Number of outliers in \"kw_avg_avg : 673\n",
            "-1.5920606553447336\n",
            "1.5979979076153152\n",
            "0.7975146407400122\n",
            "Number of outliers in \"self_reference_avg_sharess : 1376\n",
            "-0.8711713696850538\n",
            "0.5857736571323898\n",
            "0.16188278075749374\n",
            "Number of outliers in \"kw_avg_min : 635\n",
            "-2.5225310078949876\n",
            "2.7971381122145598\n",
            "0.4836062836463225\n"
          ]
        }
      ],
      "source": [
        "##  Dropping redundant columns using our previous computation using correlation matrix\n",
        "toRemove = ['n_non_stop_words', 'n_unique_tokens','kw_max_max','kw_max_min','kw_min_max','kw_max_avg','self_reference_min_shares','self_reference_max_shares','global_sentiment_polarity','global_rate_negative_words','avg_negative_polarity','abs_title_sentiment_polarity']\n",
        "df.drop(toRemove,axis=1,inplace=True)\n",
        "df_eval.drop(toRemove,axis=1,inplace=True)\n",
        "\n",
        "##  Removing outlier using Quartile. Values in the interval between Q1 - fact * IQR and Q3 + fact * IQR\n",
        "removingOutlierColumn('shares',df, fact = 8)\n",
        "removingOutlierColumn('kw_avg_avg',df,1.5)\n",
        "removingOutlierColumn(\"self_reference_avg_sharess\",df,4)\n",
        "removingOutlierColumn('kw_avg_min',df,fact = 5)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DBPs454gxxjN"
      },
      "source": [
        "HYPERPARAMETER TUNING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "cTHTaufOxz99"
      },
      "outputs": [],
      "source": [
        "y = df[\"shares\"]\n",
        "df.drop(\"shares\",axis=1,inplace=True)\n",
        "X = df\n",
        "\n",
        "# Create the Random Forest Regressor\n",
        "#rf_regressor = RandomForestRegressor(random_state = 42)\n",
        "rf_regressor = GradientBoostingRegressor()\n",
        "\n",
        "# Define the hyperparameters to tune\n",
        "parameters = {\n",
        "     'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 5, 10],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "parameters = {\n",
        "    'n_estimators': [100, 200, 300],  # Number of boosting stages\n",
        "    'learning_rate': [0.1, 0.05, 0.01],  # Learning rate\n",
        "    'max_depth': [3, 4, 5],  # Maximum depth of each tree\n",
        "}\n",
        "\n",
        "# Create the RMSE scorer\n",
        "scorer = make_scorer(lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)), greater_is_better=False)\n",
        "\n",
        "# Perform grid search with cross-validation\n",
        "grid_search = GridSearchCV(rf_regressor, parameters, scoring=scorer, cv=5)\n",
        "grid_search.fit(X, y)  # X_train and y_train are your training data\n",
        "\n",
        "# Print the best hyperparameters and RMSE score\n",
        "print(\"Best Hyperparameters: \", grid_search.best_params_)\n",
        "print(\"Best RMSE Score: \", (-grid_search.best_score_))\n",
        "\n",
        "# Get the best model\n",
        "best_rf_model = grid_search.best_estimator_\n",
        "\n",
        "print(best_rf_model)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "EVALUATION ON TRAIN SET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y = df[\"shares\"]\n",
        "\n",
        "df.drop(\"shares\",axis=1,inplace=True)\n",
        "X = df\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "rf_regressor = RandomForestRegressor(300,max_depth=35,random_state=33)\n",
        "rf_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing data\n",
        "y_pred = rf_regressor.predict(X_test)\n",
        "# Calculate RMSE\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(\"Root Mean Squared Error:\", rmse)\n",
        "df[\"shares\"] = y"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yjbAnSONv8Y7"
      },
      "source": [
        "EVALUATION ON VAL SET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qP0fjHj76hN0"
      },
      "outputs": [],
      "source": [
        "y_train = df[\"shares\"]\n",
        "df.drop(\"shares\",axis=1, inplace=True)\n",
        "X_train = df\n",
        "X_test = df_eval\n",
        "\n",
        "\n",
        "##  Initialization the model with best parameters found in training\n",
        "rf_regressor = RandomForestRegressor(n_estimators = 300, max_depth = 35, random_state = 42)\n",
        "\n",
        "rf_regressor.fit(X_train,y_train)\n",
        "\n",
        "y_pred = rf_regressor.predict(X_test)\n",
        "\n",
        "df[\"shares\"] = y_train\n",
        "\n",
        "##  Creating CSV file for submitting the results\n",
        "csv_df = pd.DataFrame(columns=['Id', 'Predicted'])\n",
        "csv_df['Id'] = id_col\n",
        "csv_df['Predicted'] = y_pred\n",
        "csv_df.to_csv('./output.csv', columns=['Id','Predicted'], index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
