{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers in \"shares : 825\n",
      "-13095.0\n",
      "16740.0\n",
      "1755.0\n",
      "Number of outliers in \"kw_avg_avg : 673\n",
      "7.170798012573379\n",
      "8.760549506070774\n",
      "0.3974378733743489\n",
      "Number of outliers in \"self_reference_avg_sharess : 1376\n",
      "-15006.458333337501\n",
      "21185.16666667\n",
      "4021.2916666675\n",
      "Number of outliers in \"kw_avg_min : 635\n",
      "0.3828502486341421\n",
      "10.46816040955948\n",
      "0.91684637826594\n",
      "Best Hyperparameters:  {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "Best RMSE Score:  2338.4820090140406\n",
      "RandomForestRegressor(max_depth=10, n_estimators=300, random_state=42)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns\n",
    "#import pingouin as pg\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def removingOutlierColumn(col,df,fact = 1.5):\n",
    "  #fact usually should be 1.5\n",
    "  q1 = df[col].quantile(0.25)    # First Quartile\n",
    "  q3 = df[col].quantile(0.75)    # Third Quartile\n",
    "  IQR = q3 - q1                            # Inter Quartile Range\n",
    "\n",
    "  llimit = q1 - fact*IQR                       # Lower Limit\n",
    "  ulimit = q3 + fact*IQR                        # Upper Limit\n",
    "\n",
    "  outliers = df[(df[col] < llimit) | (df[col] > ulimit)]\n",
    "\n",
    "  df.drop(outliers.index, axis = 0, inplace = True)\n",
    "\n",
    "\n",
    "  print('Number of outliers in \"' + col + ' : ' + str(len(outliers)))\n",
    "  print(llimit)\n",
    "  print(ulimit)\n",
    "  print(IQR)\n",
    "\n",
    "def findImportance(df):\n",
    "  X = df\n",
    "  y = df['shares']\n",
    "  feature_list = []\n",
    "  df.drop('shares', axis = 1, inplace = True)\n",
    "  reg = RandomForestRegressor(100, random_state=42)\n",
    "  reg.fit(X, y)\n",
    "  df[\"shares\"] = y\n",
    "  feature_dict = dict(sorted(zip(df.columns, reg.feature_importances_), key=lambda x: x[1],reverse=True))\n",
    "  temp = feature_dict.keys()\n",
    "  for key in temp:\n",
    "    feature_list.append(key)\n",
    "  return feature_dict,feature_list\n",
    "\n",
    "## Initializing the dataset\n",
    "path_eva = \"./data/evaluation.csv\"\n",
    "path_dev = \"./data/development.csv\"\n",
    "df = pd.read_csv(path_dev)\n",
    "df_eval = pd.read_csv(path_eva)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "id_col = df_eval['id']\n",
    "\n",
    "\n",
    "##  Deleting silly columns\n",
    "df.drop(['url','id','timedelta'], axis = 1, inplace = True)\n",
    "df_eval.drop(['url','id','timedelta'], axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "#   Remove strange outliers with some data with 0 values\n",
    "mask = (df['average_token_length'] == 0) & (df['n_tokens_content'] == 0)\n",
    "df.drop(df[mask].index, axis = 0, inplace = True)\n",
    "\n",
    "\n",
    "##Filling NaN values in num_imgs and num_videos using zero\n",
    "NaN_columns = ['num_imgs','num_videos']\n",
    "for el in NaN_columns:\n",
    "    mean_values = df.groupby('data_channel')[el].mean()\n",
    "    # Iterate over each group and fill NaN values with the corresponding mean\n",
    "    for group, mean in mean_values.items():\n",
    "      df.loc[df['data_channel'] == group, el] = df.loc[df['data_channel'] == group, el].fillna(0)\n",
    "for el in NaN_columns:\n",
    "    mean_values_eval = df.groupby('data_channel')[el].mean()\n",
    "    # Iterate over each group and fill NaN values with the corresponding mean\n",
    "    for group, mean in mean_values_eval.items():\n",
    "      df_eval.loc[df_eval['data_channel'] == group, el] = df_eval.loc[df_eval['data_channel'] == group, el].fillna(0)\n",
    "\n",
    "##  filling NaN values in num_keywords grouping by 'data_channel' and calculate the mean\n",
    "mean_values = df.groupby('data_channel')[\"num_keywords\"].mean()\n",
    "for group, mean in mean_values.items():\n",
    "  df.loc[df['data_channel'] == group, \"num_keywords\"] = df.loc[df['data_channel'] == group, \"num_keywords\"].fillna(mean)\n",
    "  df_eval.loc[df_eval['data_channel'] == group, \"num_keywords\"] = df_eval.loc[df_eval['data_channel'] == group, \"num_keywords\"].fillna(mean)\n",
    "\n",
    "##  Transforming several features into a normal distribution shape using logaritmic transformation\n",
    "logTransformation = ['n_tokens_content','num_hrefs','num_self_hrefs','num_imgs','num_videos','kw_max_min','kw_avg_min','kw_min_max','kw_min_avg','kw_max_max','kw_max_avg','kw_avg_avg','self_reference_min_shares','self_reference_max_shares']\n",
    "df[logTransformation] = np.log(1.001 + df[logTransformation])\n",
    "df_eval[logTransformation] = np.log(1.001 + df_eval[logTransformation])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "##  Z-score normalization of our data\n",
    "for el in df.columns:\n",
    "  if(el != \"shares\" and el != \"data_channel\" and el != \"weekday\"):\n",
    "    df[el] = (df[el] - df[el].mean()) / df[el].std()\n",
    "\n",
    "for el in df_eval.columns:\n",
    "  if(el != \"shares\" and el != \"data_channel\" and el != \"weekday\"):\n",
    "    df_eval[el] = (df_eval[el] - df[el].mean()) / df[el].std()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "##  One-Hot Encoding\n",
    "df = pd.get_dummies(df, columns=['data_channel','weekday'])\n",
    "df_eval = pd.get_dummies(df_eval, columns=['data_channel','weekday'])\n",
    "\n",
    "##  Dropping redundant columns using our previous computation using correlation matrix\n",
    "toRemove = ['n_non_stop_words', 'n_unique_tokens','kw_max_max','kw_max_min','kw_min_max','kw_max_avg','self_reference_min_shares','self_reference_max_shares','global_sentiment_polarity','global_rate_negative_words','avg_negative_polarity','abs_title_sentiment_polarity']\n",
    "df.drop(toRemove,axis=1,inplace=True)\n",
    "df_eval.drop(toRemove,axis=1,inplace=True)\n",
    "\n",
    "##  Removing outlier using Quartile. Values in the interval between Q1 - fact * IQR and Q3 + fact * IQR\n",
    "removingOutlierColumn('shares',df, fact = 8)\n",
    "removingOutlierColumn('kw_avg_avg',df,1.5)\n",
    "removingOutlierColumn(\"self_reference_avg_sharess\",df,4)\n",
    "removingOutlierColumn('kw_avg_min',df,fact = 5)\n",
    "\n",
    "y = df[\"shares\"]\n",
    "df.drop(\"shares\",axis=1,inplace=True)\n",
    "X = df\n",
    "\n",
    "# Create the Random Forest Regressor\n",
    "rf_regressor = RandomForestRegressor(random_state = 42)\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "parameters = {\n",
    "     'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Create the RMSE scorer\n",
    "scorer = make_scorer(lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)), greater_is_better=False)\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(rf_regressor, parameters, scoring=scorer, cv=5)\n",
    "grid_search.fit(X, y)  # X_train and y_train are your training data\n",
    "\n",
    "# Print the best hyperparameters and RMSE score\n",
    "print(\"Best Hyperparameters: \", grid_search.best_params_)\n",
    "print(\"Best RMSE Score: \", (-grid_search.best_score_))\n",
    "\n",
    "# Get the best model\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "print(best_rf_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
